{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "# --- å°‡å°ˆæ¡ˆ src/ åŠ å…¥ Python è·¯å¾‘ ---\n",
    "ROOT = Path(__file__).resolve().parents[1]\n",
    "SRC_DIR = ROOT / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "import torch\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from cyclegan_min import GeneratorResnet\n",
    "\n",
    "# ---- è·¯å¾‘èˆ‡è£ç½®è¨­å®šï¼ˆä½ ç›®å‰æ”¾åœ¨ result1/ï¼‰----\n",
    "DEFAULT_CKPT = ROOT / \"result1\" / \"checkpoints\" / \"cyclegan_ultra_epoch_01.pt\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = (DEVICE.type == \"cuda\")  # åªæœ‰ CUDA æ‰å•Ÿç”¨ AMP\n",
    "\n",
    "def _build_transform(img_size: int) -> transforms.Compose:\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "    ])\n",
    "\n",
    "def _denorm(x: torch.Tensor) -> torch.Tensor:\n",
    "    return (x * 0.5 + 0.5).clamp(0, 1)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def load_model(ckpt_path: Path = DEFAULT_CKPT) -> Tuple[torch.nn.Module, int]:\n",
    "    ckpt_path = Path(ckpt_path)\n",
    "    if not ckpt_path.exists():\n",
    "        raise FileNotFoundError(f\"æ‰¾ä¸åˆ°æ¨¡å‹æª”ï¼š{ckpt_path}\")\n",
    "    state = torch.load(ckpt_path, map_location=DEVICE)\n",
    "\n",
    "    img_size = int(state.get(\"img_size\", 64))\n",
    "    n_res = int(state.get(\"n_res\", 2))\n",
    "\n",
    "    G_AB = GeneratorResnet(n_res=n_res).to(DEVICE)\n",
    "\n",
    "    # ===== å–å‡ºä¸¦ä¿®æ­£æ¬Šé‡éµï¼Œå¯¬é¬†è¼‰å…¥ï¼ˆè§£æ±º Unexpected keys / size mismatchï¼‰=====\n",
    "    cand = None\n",
    "    for key in (\"G_AB\", \"generator_ab\", \"model\", \"state_dict\"):\n",
    "        if key in state and isinstance(state[key], dict):\n",
    "            cand = state[key]\n",
    "            break\n",
    "    if cand is None:\n",
    "        cand = state if isinstance(state, dict) else {}\n",
    "\n",
    "    # æœ‰äº› checkpoint æ¬Šé‡å¸¶ \"model.\" å‰ç¶´ â†’ å»æ‰\n",
    "    if any(isinstance(k, str) and k.startswith(\"model.\") for k in cand.keys()):\n",
    "        cand = {k.replace(\"model.\", \"\", 1): v for k, v in cand.items()}\n",
    "\n",
    "    missing, unexpected = G_AB.load_state_dict(cand, strict=False)\n",
    "    if unexpected:\n",
    "        print(\"âš ï¸  Unexpected keys ignored:\", list(unexpected)[:6], \"...\")\n",
    "    if missing:\n",
    "        print(\"âš ï¸  Missing keys (kept random init):\", list(missing)[:6], \"...\")\n",
    "\n",
    "    G_AB.eval()\n",
    "    print(f\"âœ… G_AB loaded on {DEVICE} | img_size={img_size}\")\n",
    "    return G_AB, img_size\n",
    "\n",
    "# --- å…¨åŸŸå¿«å– ---\n",
    "_G_AB = None\n",
    "_IMG_SIZE = None\n",
    "_TFM = None\n",
    "\n",
    "def _ensure_model():\n",
    "    global _G_AB, _IMG_SIZE, _TFM\n",
    "    if _G_AB is None:\n",
    "        print(\"ğŸ“‚ Loading checkpoint from:\", DEFAULT_CKPT)\n",
    "        _G_AB, _IMG_SIZE = load_model(DEFAULT_CKPT)\n",
    "        _TFM = _build_transform(_IMG_SIZE)\n",
    "\n",
    "def translate_image(pil_img: Image.Image) -> Image.Image:\n",
    "    _ensure_model()\n",
    "    x = _TFM(pil_img.convert(\"RGB\")).unsqueeze(0).to(DEVICE)\n",
    "    # é CUDA æ™‚é—œæ‰ AMPï¼Œé¿å… dtype å•é¡Œ\n",
    "    if USE_AMP:\n",
    "        with torch.amp.autocast(\"cuda\", enabled=True):\n",
    "            y = _G_AB(x)\n",
    "    else:\n",
    "        y = _G_AB(x)\n",
    "    y = _denorm(y[0]).permute(1, 2, 0).detach().cpu().numpy()\n",
    "    return Image.fromarray((y * 255).astype(\"uint8\"))\n",
    "\n",
    "def app():\n",
    "    title = \"Ancient â†’ Film Style (CycleGAN Demo)\"\n",
    "    description = (\n",
    "        \"ä¸Šå‚³ä¸€å¼µå¤ç•«é¢¨æ ¼åœ–åƒï¼ˆdomain Aï¼‰ï¼Œæ¨¡å‹å°‡è¼¸å‡ºé›»å½±é¢¨æ ¼åœ–åƒï¼ˆdomain Bï¼‰ã€‚\"\n",
    "        f\"<br>Device: <b>{DEVICE.type.upper()}</b> | Using AMP: <b>{USE_AMP}</b>\"\n",
    "        f\"<br>Checkpoint: <code>{DEFAULT_CKPT.relative_to(ROOT)}</code>\"\n",
    "    )\n",
    "    return gr.Interface(\n",
    "        fn=translate_image,\n",
    "        inputs=gr.Image(type=\"pil\", label=\"Upload ancient-style image (A)\"),\n",
    "        outputs=gr.Image(type=\"pil\", label=\"Generated film-style image (B)\"),\n",
    "        title=title,\n",
    "        description=description,\n",
    "        allow_flagging=\"never\",\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app().queue().launch(server_name=\"127.0.0.1\", server_port=7860)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
