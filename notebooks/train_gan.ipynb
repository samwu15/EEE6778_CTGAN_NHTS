{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe696dcb",
   "metadata": {},
   "source": [
    "# Ancient-to-Film GAN ‚Äî CycleGAN Training Notebook\n",
    "**Deliverable 2 | Implementation & Early Evaluation**\n",
    "\n",
    "This notebook provides a minimal, end-to-end CycleGAN training scaffold for unpaired image-to-image translation.\n",
    "- **Domain A**: Ancient paintings (`data/A`)\n",
    "- **Domain B**: Film-style photos (`data/B`)\n",
    "\n",
    "> Tip: Start with small images (128√ó128) and a few epochs to verify the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712eaf55",
   "metadata": {},
   "source": [
    "## 0. Environment & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cddf93",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mÂü∑Ë°åÂÖ∑Êúâ 'aml (Python 3.8.20)' ÁöÑÂÑ≤Â≠òÊ†ºÈúÄË¶Å ipykernel Â•ó‰ª∂„ÄÇ\n",
      "\u001b[1;31mÂü∑Ë°å‰∏ãÂàóÂëΩ‰ª§‰ª•Â∞á 'ipykernel' ÂÆâË£ùÂà∞ Python Áí∞Â¢É‰∏≠„ÄÇ\n",
      "\u001b[1;31mÂëΩ‰ª§: 'conda install -n aml ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, random, itertools, time, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# Paths\n",
    "ROOT = Path(\"..\").resolve() if (Path.cwd().name == \"notebooks\") else Path(\".\").resolve()\n",
    "DATA_A = ROOT / \"data\" / \"A\"\n",
    "DATA_B = ROOT / \"data\" / \"B\"\n",
    "OUT_SAMPLES = ROOT / \"results\" / \"samples\"\n",
    "OUT_CKPTS = ROOT / \"results\" / \"checkpoints\"\n",
    "OUT_SAMPLES.mkdir(parents=True, exist_ok=True)\n",
    "OUT_CKPTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Hyperparameters (start small for demo)\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 5\n",
    "LR_G = 2e-4\n",
    "LR_D = 2e-4\n",
    "LAMBDA_CYCLE = 10.0\n",
    "LAMBDA_ID = 5.0\n",
    "NUM_WORKERS = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea8a3c1",
   "metadata": {},
   "source": [
    "## 1. Dataset (Unpaired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb170b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Robust Dataset Setup (safe version) ---\n",
    "import random\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Ëã•Áí∞Â¢É cell Ê≤íË∑ëÔºåÁµ¶È†êË®≠ÂÄº\n",
    "IMG_SIZE = globals().get(\"IMG_SIZE\", 128)\n",
    "BATCH_SIZE = globals().get(\"BATCH_SIZE\", 2)\n",
    "NUM_WORKERS = 0  # üëà Âú® macOS / Windows Âª∫Ë≠∞ÂÖàÁî® 0ÔºåÈÅøÂÖçÂ§öÈÄ≤Á®ãÂïèÈ°å\n",
    "\n",
    "# Ê†πÁõÆÈåÑÊé®Êñ∑ÔºàNotebookÈÄöÂ∏∏Âú® notebooks/ ÂÖßÔºâ\n",
    "ROOT = Path(\"..\").resolve() if (Path.cwd().name == \"notebooks\") else Path(\".\").resolve()\n",
    "DATA_A = ROOT / \"data\" / \"A\"\n",
    "DATA_B = ROOT / \"data\" / \"B\"\n",
    "print(\"ROOT =\", ROOT)\n",
    "print(\"DATA_A exists:\", DATA_A.exists(), \"| DATA_B exists:\", DATA_B.exists())\n",
    "\n",
    "# Ê™¢Êü•ÂúñÊ™îÊï∏Èáè\n",
    "def count_images(p):\n",
    "    exts = [\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.JPEG\",\"*.PNG\"]\n",
    "    files = []\n",
    "    for e in exts:\n",
    "        files += list(p.glob(e))\n",
    "    return len(files)\n",
    "\n",
    "cntA = count_images(DATA_A)\n",
    "cntB = count_images(DATA_B)\n",
    "print(f\"Found images -> A: {cntA} | B: {cntB}\")\n",
    "assert cntA > 0 and cntB > 0, \"data/A Êàñ data/B Ê≤íÊúâÊâæÂà∞ÂúñÊ™îÔºà.jpg/.pngÔºâ„ÄÇ\"\n",
    "\n",
    "# Ë≥áÊñôËΩâÊèõ\n",
    "tfm = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]),\n",
    "])\n",
    "\n",
    "class UnpairedImageDataset(Dataset):\n",
    "    def __init__(self, dir_a, dir_b, transform=None):\n",
    "        exts = {\".jpg\",\".jpeg\",\".png\",\".JPG\",\".JPEG\",\".PNG\"}\n",
    "        self.paths_a = sorted([p for p in Path(dir_a).glob(\"*\") if p.suffix in exts])\n",
    "        self.paths_b = sorted([p for p in Path(dir_b).glob(\"*\") if p.suffix in exts])\n",
    "        self.transform = transform\n",
    "        if len(self.paths_a) == 0 or len(self.paths_b) == 0:\n",
    "            raise RuntimeError(\"Ë´ãÁ¢∫Ë™ç data/A Ëàá data/B ÂÖßÊúâÂúñÊ™îÔºà.jpg/.pngÔºâ„ÄÇ\")\n",
    "    def __len__(self):\n",
    "        return max(len(self.paths_a), len(self.paths_b))\n",
    "    def __getitem__(self, idx):\n",
    "        pa = self.paths_a[idx % len(self.paths_a)]\n",
    "        pb = self.paths_b[random.randint(0, len(self.paths_b)-1)]\n",
    "        ia, ib = Image.open(pa).convert(\"RGB\"), Image.open(pb).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            ia = self.transform(ia)\n",
    "            ib = self.transform(ib)\n",
    "        return ia, ib\n",
    "\n",
    "ds = UnpairedImageDataset(DATA_A, DATA_B, transform=tfm)\n",
    "# drop_last=True ÂèØÈÅøÂÖçÊúÄÂæå‰∏ÄÂÄã batch ‰∏çË∂≥ÈÄ†Êàê shape ÂïèÈ°å\n",
    "dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
    "print(\"‚úÖ DataLoader Âª∫Á´ãÊàêÂäüÔºådataset Èï∑Â∫¶ =\", len(ds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a81d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Executable:\", sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, PIL, torch, torchvision\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"Pillow:\", PIL.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"torchvision:\", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e8e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch, torchvision, PIL\n",
    "print(np.__version__)      # ÊúüÊúõ 1.26.4\n",
    "print(torch.__version__)   # 2.2.2\n",
    "print(torchvision.__version__)  # 0.17.2\n",
    "x = torch.from_numpy(np.zeros((3,3), dtype=np.float32))  # ‰∏çË©≤Â†±ÈåØ\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26149585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch, torchvision, PIL\n",
    "print(\"numpy:\", np.__version__)          # 2.xÔºàÂèØ‰øùÁïôÔºâ\n",
    "print(\"torch:\", torch.__version__)       # 2.4.xÔºàÊàñÊõ¥È´ò‰ΩÜ <2.6Ôºâ\n",
    "print(\"torchvision:\", torchvision.__version__)  # 0.19.xÔºàÊàñ <0.21Ôºâ\n",
    "print(\"pillow:\", PIL.__version__)\n",
    "import numpy as _np, torch as _t\n",
    "print(_t.from_numpy(_np.zeros((2,2), dtype=_np.float32)).shape)  # ÊáâËº∏Âá∫ torch.Size([2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec789ad",
   "metadata": {},
   "source": [
    "### Preview a mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41533856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "def denorm(x):\n",
    "    return (x * 0.5 + 0.5).clamp(0,1)\n",
    "\n",
    "# Á¢∫‰øù dl Â≠òÂú®\n",
    "if 'dl' not in globals():\n",
    "    raise RuntimeError(\"DataLoader (dl) Â∞öÊú™Âª∫Á´ãÔºåË´ãÂÖàÂü∑Ë°å Dataset ÁöÑ cell„ÄÇ\")\n",
    "\n",
    "batch = next(iter(dl))\n",
    "a_batch, b_batch = batch\n",
    "print(\"Batch shapes:\", a_batch.shape, b_batch.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1, a_batch.shape[0]*2, figsize=(12, 3))\n",
    "for i in range(a_batch.shape[0]):\n",
    "    axes[2*i].imshow(denorm(a_batch[i]).permute(1,2,0).numpy())\n",
    "    axes[2*i].set_title(\"A (Ancient)\"); axes[2*i].axis(\"off\")\n",
    "    axes[2*i+1].imshow(denorm(b_batch[i]).permute(1,2,0).numpy())\n",
    "    axes[2*i+1].set_title(\"B (Film)\"); axes[2*i+1].axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a48a782",
   "metadata": {},
   "source": [
    "## 2. Models (Generator & Discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecde8c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.cyclegan_min import GeneratorResnet, DiscriminatorPatchGAN\n",
    "\n",
    "# Generators: A->B (G_AB) and B->A (G_BA)\n",
    "G_AB = GeneratorResnet().to(DEVICE)\n",
    "G_BA = GeneratorResnet().to(DEVICE)\n",
    "\n",
    "# Discriminators: for domain B (D_B) and domain A (D_A)\n",
    "D_B = DiscriminatorPatchGAN().to(DEVICE)\n",
    "D_A = DiscriminatorPatchGAN().to(DEVICE)\n",
    "\n",
    "# Init weights\n",
    "def init_weights(m):\n",
    "    if isinstance(m, (torch.nn.Conv2d, torch.nn.ConvTranspose2d)):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, torch.nn.InstanceNorm2d):\n",
    "        if m.affine:\n",
    "            torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "G_AB.apply(init_weights); G_BA.apply(init_weights); D_A.apply(init_weights); D_B.apply(init_weights)\n",
    "\n",
    "# Losses & Optimizers\n",
    "mse = nn.MSELoss()\n",
    "l1  = nn.L1Loss()\n",
    "opt_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=LR_G, betas=(0.5, 0.999))\n",
    "opt_D_A = torch.optim.Adam(D_A.parameters(), lr=LR_D, betas=(0.5, 0.999))\n",
    "opt_D_B = torch.optim.Adam(D_B.parameters(), lr=LR_D, betas=(0.5, 0.999))\n",
    "\n",
    "# Buffers for GAN targets\n",
    "def real_like(x): return torch.ones_like(x, device=DEVICE)\n",
    "def fake_like(x): return torch.zeros_like(x, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b1be43",
   "metadata": {},
   "source": [
    "## 3. Training Loop (Toy Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca9e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_sample(a, b_fake, step, out_dir=OUT_SAMPLES):\n",
    "    a = denorm(a.detach().cpu())\n",
    "    b_fake = denorm(b_fake.detach().cpu())\n",
    "    grid = torch.cat([a, b_fake], dim=0)  # stack\n",
    "    # save first pair\n",
    "    a0 = (a[0].permute(1,2,0).numpy()*255).astype(\"uint8\")\n",
    "    b0 = (b_fake[0].permute(1,2,0).numpy()*255).astype(\"uint8\")\n",
    "    Image.fromarray(a0).save(out_dir / f\"step{step:06d}_A.png\")\n",
    "    Image.fromarray(b0).save(out_dir / f\"step{step:06d}_AtoB.png\")\n",
    "\n",
    "step = 0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    pbar = tqdm(dl, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)\n",
    "    for a, b in pbar:\n",
    "        a = a.to(DEVICE); b = b.to(DEVICE)\n",
    "\n",
    "        # --------------------\n",
    "        #  Train Generators\n",
    "        # --------------------\n",
    "        opt_G.zero_grad()\n",
    "\n",
    "        b_fake = G_AB(a)\n",
    "        a_rec  = G_BA(b_fake)\n",
    "        a_fake = G_BA(b)\n",
    "        b_rec  = G_AB(a_fake)\n",
    "\n",
    "        # identity loss (optional but stabilizes)\n",
    "        loss_id_a = l1(a_fake, b) * LAMBDA_ID\n",
    "        loss_id_b = l1(b_fake, a) * LAMBDA_ID\n",
    "\n",
    "        # adversarial\n",
    "        pred_b = D_B(b_fake)\n",
    "        pred_a = D_A(a_fake)\n",
    "        loss_gan_ab = mse(pred_b, real_like(pred_b))\n",
    "        loss_gan_ba = mse(pred_a, real_like(pred_a))\n",
    "\n",
    "        # cycle-consistency\n",
    "        loss_cyc_a = l1(a_rec, a) * LAMBDA_CYCLE\n",
    "        loss_cyc_b = l1(b_rec, b) * LAMBDA_CYCLE\n",
    "\n",
    "        loss_G = loss_id_a + loss_id_b + loss_gan_ab + loss_gan_ba + loss_cyc_a + loss_cyc_b\n",
    "        loss_G.backward()\n",
    "        opt_G.step()\n",
    "\n",
    "        # --------------------\n",
    "        #  Train D_A\n",
    "        # --------------------\n",
    "        opt_D_A.zero_grad()\n",
    "        pred_real_a = D_A(a)\n",
    "        pred_fake_a = D_A(a_fake.detach())\n",
    "        loss_D_A = (mse(pred_real_a, real_like(pred_real_a)) + mse(pred_fake_a, fake_like(pred_fake_a))) * 0.5\n",
    "        loss_D_A.backward()\n",
    "        opt_D_A.step()\n",
    "\n",
    "        # --------------------\n",
    "        #  Train D_B\n",
    "        # --------------------\n",
    "        opt_D_B.zero_grad()\n",
    "        pred_real_b = D_B(b)\n",
    "        pred_fake_b = D_B(b_fake.detach())\n",
    "        loss_D_B = (mse(pred_real_b, real_like(pred_real_b)) + mse(pred_fake_b, fake_like(pred_fake_b))) * 0.5\n",
    "        loss_D_B.backward()\n",
    "        opt_D_B.step()\n",
    "\n",
    "        step += 1\n",
    "        if step % 50 == 0:\n",
    "            save_sample(a, b_fake, step)\n",
    "        pbar.set_postfix({\n",
    "            \"G\": f\"{loss_G.item():.3f}\",\n",
    "            \"D_A\": f\"{loss_D_A.item():.3f}\",\n",
    "            \"D_B\": f\"{loss_D_B.item():.3f}\"\n",
    "        })\n",
    "\n",
    "    # save checkpoint each epoch\n",
    "    torch.save({\n",
    "        \"G_AB\": G_AB.state_dict(),\n",
    "        \"G_BA\": G_BA.state_dict(),\n",
    "        \"D_A\": D_A.state_dict(),\n",
    "        \"D_B\": D_B.state_dict(),\n",
    "        \"epoch\": epoch\n",
    "    }, OUT_CKPTS / f\"cyclegan_epoch_{epoch:02d}.pt\")\n",
    "    print(f\"[Epoch {epoch}] checkpoint saved.\")\n",
    "print(\"Training loop finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be026db3",
   "metadata": {},
   "source": [
    "## 4. Inference Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.inference_mode()\n",
    "def translate_image(path_in, path_out, ckpt=None, direction=\"A2B\"):\n",
    "    img = Image.open(path_in).convert(\"RGB\")\n",
    "    x = tfm(img).unsqueeze(0).to(DEVICE)\n",
    "    if ckpt:\n",
    "        state = torch.load(ckpt, map_location=DEVICE)\n",
    "        G_AB.load_state_dict(state[\"G_AB\"]); G_BA.load_state_dict(state[\"G_BA\"])\n",
    "    if direction == \"A2B\":\n",
    "        y = G_AB(x)\n",
    "    else:\n",
    "        y = G_BA(x)\n",
    "    y = denorm(y[0].cpu()).permute(1,2,0).numpy()\n",
    "    Image.fromarray((y*255).astype(\"uint8\")).save(path_out)\n",
    "    return path_out\n",
    "\n",
    "# Example:\n",
    "# translate_image(ROOT/'data/A/sample.jpg', ROOT/'results/samples/sample_A2B.png', ckpt=ROOT/'results/checkpoints/cyclegan_epoch_01.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69646bff",
   "metadata": {},
   "source": [
    "## 5. Notes for Deliverable 2 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc92781",
   "metadata": {},
   "source": [
    "\n",
    "- Keep **EPOCHS small** (e.g., 3‚Äì5) and **IMG_SIZE=128** for a quick demo.\n",
    "- Save a few outputs in `results/samples/` and insert them into your report.\n",
    "- Record losses per epoch; optionally add a simple loss curve.\n",
    "- Mention compute setup (CPU/GPU), and any training instability or artifacts observed.\n",
    "- For interface (Step 3), load the latest checkpoint in `ui/app.py` and call `translate_image`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
